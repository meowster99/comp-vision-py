{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Pillow\n",
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install scipy\n",
    "# !pip install tensorflow\n",
    "# !pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Different Batch Size and Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'data'\n",
    "mask_dir = os.path.join(base_dir, 'with_mask')\n",
    "wo_mask_dir = os.path.join(base_dir, 'without_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of mask pictures: 3725\n",
      "number of mask pictures: 3828\n"
     ]
    }
   ],
   "source": [
    "mask_files = list(os.walk(mask_dir))[0][2]\n",
    "print(f'number of mask pictures: {len(mask_files)}')\n",
    "wo_mask_files = list(os.walk(wo_mask_dir))[0][2]\n",
    "print(f'number of mask pictures: {len(wo_mask_files)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_minibatch_learning_rate(mini_batch_size, learning_rate,epochs=5):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        # Note the input shape is the desired size of the image 224x224 with 3 bytes color\n",
    "        # This is the first convolution\n",
    "        tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(224, 224, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        # The second convolution\n",
    "        tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        # The third convolution\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        # The fourth convolution\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        # The fifth convolution\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        # Flatten the results to feed into a DNN\n",
    "        tf.keras.layers.Flatten(),\n",
    "        # 512 neuron hidden layer\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('with mask') and 1 for the other ('without mask')\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                optimizer=RMSprop(learning_rate=learning_rate),\n",
    "                metrics=['accuracy'])\n",
    "    batch_size = mini_batch_size\n",
    "    # All images will be rescaled by 1./255\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255, \n",
    "                                    rotation_range = 40,\n",
    "                                    width_shift_range = 0.2,\n",
    "                                    height_shift_range = 0.2,\n",
    "                                    shear_range = 0.2,\n",
    "                                    zoom_range = 0.2,\n",
    "                                    horizontal_flip = True,\n",
    "                                    brightness_range=[0.4,1.5],\n",
    "                                    validation_split=0.2) # 20% split and it is not random\n",
    "\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2) # created another generator for validation without augmentation\n",
    "\n",
    "    # Flow training images in batches of 128 using train_datagen generator\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "            base_dir,  # This is the source directory for training images\n",
    "            target_size=(224, 224),  # All images will be resized to 224x224\n",
    "            batch_size=batch_size,\n",
    "            # Since we use binary_crossentropy loss, we need binary labels\n",
    "            class_mode='binary',\n",
    "            shuffle=True,\n",
    "            subset='training') #set as training data\n",
    "\n",
    "    validation_generator = val_datagen.flow_from_directory(\n",
    "            base_dir,  # This is the source directory for training images\n",
    "            target_size=(224, 224),  # All images will be resized to 224x224\n",
    "            batch_size=batch_size,\n",
    "            # Since we use binary_crossentropy loss, we need binary labels\n",
    "            class_mode='binary',\n",
    "            shuffle=False,\n",
    "            subset='validation') #set as validation data\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=train_generator.samples // batch_size,  \n",
    "        validation_data = validation_generator,\n",
    "        validation_steps = validation_generator.samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        initial_epoch=0,\n",
    "        verbose=1)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6043 images belonging to 2 classes.\n",
      "Found 1510 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "38/47 [=======================>......] - ETA: 1:00 - loss: 0.6935 - accuracy: 0.5872"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bao\\Desktop\\comp-vision-py\\project_env\\lib\\site-packages\\PIL\\Image.py:945: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 328s 7s/step - loss: 0.6725 - accuracy: 0.6166 - val_loss: 0.5080 - val_accuracy: 0.9126\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 285s 6s/step - loss: 0.4941 - accuracy: 0.7772 - val_loss: 0.3645 - val_accuracy: 0.9276\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 280s 6s/step - loss: 0.4488 - accuracy: 0.7981 - val_loss: 0.2736 - val_accuracy: 0.9439\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 304s 6s/step - loss: 0.4058 - accuracy: 0.8221 - val_loss: 0.2034 - val_accuracy: 0.9290\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 299s 6s/step - loss: 0.3792 - accuracy: 0.8399 - val_loss: 0.1919 - val_accuracy: 0.9332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1772dc5eee0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_minibatch_learning_rate(128, 0.001, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6043 images belonging to 2 classes.\n",
      "Found 1510 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 311s 7s/step - loss: 171.1599 - accuracy: 0.6385 - val_loss: 0.2754 - val_accuracy: 0.9112\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 317s 7s/step - loss: 0.5668 - accuracy: 0.7374 - val_loss: 0.9365 - val_accuracy: 0.6697\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 291s 6s/step - loss: 0.7195 - accuracy: 0.5055 - val_loss: 0.6929 - val_accuracy: 0.5291\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 285s 6s/step - loss: 0.6935 - accuracy: 0.5033 - val_loss: 0.6959 - val_accuracy: 0.4709\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 284s 6s/step - loss: 0.6935 - accuracy: 0.4920 - val_loss: 0.6924 - val_accuracy: 0.5291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1772e2ebf40>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_minibatch_learning_rate(128, 0.01, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6043 images belonging to 2 classes.\n",
      "Found 1510 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 306s 6s/step - loss: 0.6218 - accuracy: 0.6600 - val_loss: 0.4692 - val_accuracy: 0.8224\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 31726s 690s/step - loss: 0.5065 - accuracy: 0.7686 - val_loss: 0.3485 - val_accuracy: 0.8757\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 273s 6s/step - loss: 0.4735 - accuracy: 0.7897 - val_loss: 0.2985 - val_accuracy: 0.8949\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 277s 6s/step - loss: 0.4492 - accuracy: 0.8024 - val_loss: 0.2807 - val_accuracy: 0.9048\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 268s 6s/step - loss: 0.4347 - accuracy: 0.8107 - val_loss: 0.2603 - val_accuracy: 0.8970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1772e7af430>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_minibatch_learning_rate(128, 0.0001, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6043 images belonging to 2 classes.\n",
      "Found 1510 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "94/94 [==============================] - 274s 3s/step - loss: 0.5875 - accuracy: 0.7102 - val_loss: 0.3073 - val_accuracy: 0.8981\n",
      "Epoch 2/5\n",
      "94/94 [==============================] - 277s 3s/step - loss: 0.4512 - accuracy: 0.7931 - val_loss: 0.2451 - val_accuracy: 0.9185\n",
      "Epoch 3/5\n",
      "94/94 [==============================] - 326s 3s/step - loss: 0.4011 - accuracy: 0.8262 - val_loss: 0.2167 - val_accuracy: 0.9382\n",
      "Epoch 4/5\n",
      "94/94 [==============================] - 340s 4s/step - loss: 0.3516 - accuracy: 0.8468 - val_loss: 0.1710 - val_accuracy: 0.9484\n",
      "Epoch 5/5\n",
      "94/94 [==============================] - 2335s 25s/step - loss: 0.3243 - accuracy: 0.8630 - val_loss: 0.1558 - val_accuracy: 0.9497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x177311cbd90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_minibatch_learning_rate(64, 0.0005, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6043 images belonging to 2 classes.\n",
      "Found 1510 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "94/94 [==============================] - 150s 2s/step - loss: 0.5924 - accuracy: 0.7063 - val_loss: 0.3452 - val_accuracy: 0.9212\n",
      "Epoch 2/5\n",
      "94/94 [==============================] - 152s 2s/step - loss: 0.4219 - accuracy: 0.8090 - val_loss: 0.2487 - val_accuracy: 0.9117\n",
      "Epoch 3/5\n",
      "94/94 [==============================] - 152s 2s/step - loss: 0.3615 - accuracy: 0.8480 - val_loss: 0.1956 - val_accuracy: 0.9212\n",
      "Epoch 4/5\n",
      "94/94 [==============================] - 155s 2s/step - loss: 0.3224 - accuracy: 0.8660 - val_loss: 0.1511 - val_accuracy: 0.9416\n",
      "Epoch 5/5\n",
      "94/94 [==============================] - 149s 2s/step - loss: 0.3154 - accuracy: 0.8710 - val_loss: 0.1868 - val_accuracy: 0.9463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x177316b0880>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_minibatch_learning_rate(64, 0.001, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6043 images belonging to 2 classes.\n",
      "Found 1510 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "23/23 [==============================] - 142s 6s/step - loss: 0.8104 - accuracy: 0.5789 - val_loss: 0.4890 - val_accuracy: 0.8719\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 145s 6s/step - loss: 0.5077 - accuracy: 0.7664 - val_loss: 0.5301 - val_accuracy: 0.7094\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 145s 6s/step - loss: 0.5116 - accuracy: 0.7655 - val_loss: 0.3008 - val_accuracy: 0.9133\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 144s 6s/step - loss: 0.4566 - accuracy: 0.7970 - val_loss: 0.2320 - val_accuracy: 0.9195\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 140s 6s/step - loss: 0.4328 - accuracy: 0.8025 - val_loss: 0.2090 - val_accuracy: 0.9344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x177361f7670>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_minibatch_learning_rate(256, 0.001, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6043 images belonging to 2 classes.\n",
      "Found 1510 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "23/23 [==============================] - 145s 6s/step - loss: 1.5519 - accuracy: 0.5531 - val_loss: 0.4914 - val_accuracy: 0.8578\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 144s 6s/step - loss: 0.6385 - accuracy: 0.6964 - val_loss: 0.6571 - val_accuracy: 0.5898\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 152s 7s/step - loss: 0.6680 - accuracy: 0.6789 - val_loss: 0.6889 - val_accuracy: 0.5570\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 156s 7s/step - loss: 0.5986 - accuracy: 0.7208 - val_loss: 0.6710 - val_accuracy: 0.5547\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 153s 7s/step - loss: 0.5662 - accuracy: 0.7301 - val_loss: 0.4302 - val_accuracy: 0.8727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17737a569d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_minibatch_learning_rate(256, 0.002, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6043 images belonging to 2 classes.\n",
      "Found 1510 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "23/23 [==============================] - 151s 6s/step - loss: 1.1114 - accuracy: 0.5440 - val_loss: 0.7957 - val_accuracy: 0.5820\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 143s 6s/step - loss: 0.6424 - accuracy: 0.6473 - val_loss: 0.6992 - val_accuracy: 0.5453\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 144s 6s/step - loss: 0.6365 - accuracy: 0.6627 - val_loss: 0.6413 - val_accuracy: 0.6352\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 147s 6s/step - loss: 0.6305 - accuracy: 0.7031 - val_loss: 0.2809 - val_accuracy: 0.9102\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 146s 6s/step - loss: 0.5245 - accuracy: 0.7719 - val_loss: 0.2779 - val_accuracy: 0.9312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1773834b9d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_minibatch_learning_rate(256, 0.00141, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini batch size of 64 seems to perform the best\n",
    "\n",
    "Trying out different learning rate for a batch size of 64 for 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6043 images belonging to 2 classes.\n",
      "Found 1510 images belonging to 2 classes.\n",
      "Epoch 1/20\n",
      "94/94 [==============================] - 148s 2s/step - loss: 0.5372 - accuracy: 0.7428 - val_loss: 0.2426 - val_accuracy: 0.9321\n",
      "Epoch 2/20\n",
      "94/94 [==============================] - 168s 2s/step - loss: 0.4005 - accuracy: 0.8254 - val_loss: 0.2071 - val_accuracy: 0.9212\n",
      "Epoch 3/20\n",
      "94/94 [==============================] - 171s 2s/step - loss: 0.3569 - accuracy: 0.8476 - val_loss: 0.1662 - val_accuracy: 0.9416\n",
      "Epoch 4/20\n",
      "94/94 [==============================] - 149s 2s/step - loss: 0.3205 - accuracy: 0.8629 - val_loss: 0.1718 - val_accuracy: 0.9450\n",
      "Epoch 5/20\n",
      "94/94 [==============================] - 155s 2s/step - loss: 0.3074 - accuracy: 0.8690 - val_loss: 0.1361 - val_accuracy: 0.9511\n",
      "Epoch 6/20\n",
      "94/94 [==============================] - 156s 2s/step - loss: 0.2870 - accuracy: 0.8796 - val_loss: 0.1616 - val_accuracy: 0.9524\n",
      "Epoch 7/20\n",
      "94/94 [==============================] - 159s 2s/step - loss: 0.2743 - accuracy: 0.8874 - val_loss: 0.1105 - val_accuracy: 0.9654\n",
      "Epoch 8/20\n",
      "94/94 [==============================] - 177s 2s/step - loss: 0.2528 - accuracy: 0.9032 - val_loss: 0.1056 - val_accuracy: 0.9586\n",
      "Epoch 9/20\n",
      "94/94 [==============================] - 316s 3s/step - loss: 0.2344 - accuracy: 0.9080 - val_loss: 0.3018 - val_accuracy: 0.8852\n",
      "Epoch 10/20\n",
      "94/94 [==============================] - 346s 4s/step - loss: 0.2475 - accuracy: 0.9023 - val_loss: 0.1167 - val_accuracy: 0.9626\n",
      "Epoch 11/20\n",
      "94/94 [==============================] - 1082s 12s/step - loss: 0.2224 - accuracy: 0.9165 - val_loss: 0.1047 - val_accuracy: 0.9660\n",
      "Epoch 12/20\n",
      "94/94 [==============================] - 246s 3s/step - loss: 0.2153 - accuracy: 0.9199 - val_loss: 0.1613 - val_accuracy: 0.9402\n",
      "Epoch 13/20\n",
      "94/94 [==============================] - 246s 3s/step - loss: 0.2000 - accuracy: 0.9239 - val_loss: 0.0833 - val_accuracy: 0.9688\n",
      "Epoch 14/20\n",
      "94/94 [==============================] - 283s 3s/step - loss: 0.1959 - accuracy: 0.9256 - val_loss: 0.0760 - val_accuracy: 0.9728\n",
      "Epoch 15/20\n",
      "94/94 [==============================] - 286s 3s/step - loss: 0.1835 - accuracy: 0.9314 - val_loss: 0.1076 - val_accuracy: 0.9626\n",
      "Epoch 16/20\n",
      "94/94 [==============================] - 360s 4s/step - loss: 0.1808 - accuracy: 0.9341 - val_loss: 0.0787 - val_accuracy: 0.9701\n",
      "Epoch 17/20\n",
      "94/94 [==============================] - 462s 5s/step - loss: 0.1760 - accuracy: 0.9358 - val_loss: 0.0642 - val_accuracy: 0.9769\n",
      "Epoch 18/20\n",
      "94/94 [==============================] - 452s 5s/step - loss: 0.1658 - accuracy: 0.9403 - val_loss: 0.1232 - val_accuracy: 0.9538\n",
      "Epoch 19/20\n",
      "94/94 [==============================] - 449s 5s/step - loss: 0.1570 - accuracy: 0.9440 - val_loss: 0.0728 - val_accuracy: 0.9735\n",
      "Epoch 20/20\n",
      "94/94 [==============================] - 265s 3s/step - loss: 0.1548 - accuracy: 0.9443 - val_loss: 0.0664 - val_accuracy: 0.9762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1772e687e80>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_minibatch_learning_rate(64, 0.0005, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6043 images belonging to 2 classes.\n",
      "Found 1510 images belonging to 2 classes.\n",
      "Epoch 1/20\n",
      "94/94 [==============================] - 184s 2s/step - loss: 0.6269 - accuracy: 0.6670 - val_loss: 0.3599 - val_accuracy: 0.8974\n",
      "Epoch 2/20\n",
      "94/94 [==============================] - 187s 2s/step - loss: 0.4657 - accuracy: 0.7881 - val_loss: 0.2203 - val_accuracy: 0.9253\n",
      "Epoch 3/20\n",
      "94/94 [==============================] - 187s 2s/step - loss: 0.3956 - accuracy: 0.8307 - val_loss: 0.1685 - val_accuracy: 0.9450\n",
      "Epoch 4/20\n",
      "94/94 [==============================] - 184s 2s/step - loss: 0.3539 - accuracy: 0.8570 - val_loss: 0.1756 - val_accuracy: 0.9382\n",
      "Epoch 5/20\n",
      "94/94 [==============================] - 166s 2s/step - loss: 0.3300 - accuracy: 0.8694 - val_loss: 0.1749 - val_accuracy: 0.9348\n",
      "Epoch 6/20\n",
      "94/94 [==============================] - 167s 2s/step - loss: 0.2999 - accuracy: 0.8814 - val_loss: 0.1425 - val_accuracy: 0.9538\n",
      "Epoch 7/20\n",
      "94/94 [==============================] - 172s 2s/step - loss: 0.2851 - accuracy: 0.8806 - val_loss: 0.2621 - val_accuracy: 0.9198\n",
      "Epoch 8/20\n",
      "94/94 [==============================] - 159s 2s/step - loss: 0.2603 - accuracy: 0.8985 - val_loss: 0.1347 - val_accuracy: 0.9586\n",
      "Epoch 9/20\n",
      "94/94 [==============================] - 162s 2s/step - loss: 0.2428 - accuracy: 0.9070 - val_loss: 0.1284 - val_accuracy: 0.9490\n",
      "Epoch 10/20\n",
      "94/94 [==============================] - 160s 2s/step - loss: 0.2227 - accuracy: 0.9134 - val_loss: 0.1192 - val_accuracy: 0.9565\n",
      "Epoch 11/20\n",
      "94/94 [==============================] - 147s 2s/step - loss: 0.2126 - accuracy: 0.9187 - val_loss: 0.1020 - val_accuracy: 0.9640\n",
      "Epoch 12/20\n",
      "94/94 [==============================] - 180s 2s/step - loss: 0.2078 - accuracy: 0.9226 - val_loss: 0.1072 - val_accuracy: 0.9524\n",
      "Epoch 13/20\n",
      "94/94 [==============================] - 256s 3s/step - loss: 0.1941 - accuracy: 0.9293 - val_loss: 0.0879 - val_accuracy: 0.9688\n",
      "Epoch 14/20\n",
      "94/94 [==============================] - 211s 2s/step - loss: 0.1879 - accuracy: 0.9303 - val_loss: 0.1720 - val_accuracy: 0.9266\n",
      "Epoch 15/20\n",
      "94/94 [==============================] - 166s 2s/step - loss: 0.1820 - accuracy: 0.9346 - val_loss: 0.1061 - val_accuracy: 0.9599\n",
      "Epoch 16/20\n",
      "94/94 [==============================] - 165s 2s/step - loss: 0.1653 - accuracy: 0.9354 - val_loss: 0.0665 - val_accuracy: 0.9728\n",
      "Epoch 17/20\n",
      "94/94 [==============================] - 158s 2s/step - loss: 0.1602 - accuracy: 0.9395 - val_loss: 0.0660 - val_accuracy: 0.9769\n",
      "Epoch 18/20\n",
      "94/94 [==============================] - 148s 2s/step - loss: 0.1523 - accuracy: 0.9475 - val_loss: 0.0758 - val_accuracy: 0.9735\n",
      "Epoch 19/20\n",
      "94/94 [==============================] - 143s 2s/step - loss: 0.1559 - accuracy: 0.9448 - val_loss: 0.0724 - val_accuracy: 0.9762\n",
      "Epoch 20/20\n",
      "94/94 [==============================] - 148s 2s/step - loss: 0.1550 - accuracy: 0.9446 - val_loss: 0.0680 - val_accuracy: 0.9762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x177395cd2e0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_minibatch_learning_rate(64, 0.001, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "both learning rate seem to perform similarly but the lower learning rate seem to perform better at the start so we will be going with a learning rate of 0.0005 and a mini batch size of 64"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "82e664c9fb90cb86e01bde49e7a5aef54607157a4682a9c5e873aa7e268f2cf3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
